{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3 (ipykernel)",
   "language": "python"
  },
  "language_info": {
   "name": "python"
  }
 },
 "cells": [
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "sUQC2YGDMPaS",
    "outputId": "694c7198-bf75-4eb0-f7e4-6b484527de06",
    "ExecuteTime": {
     "end_time": "2024-06-12T22:00:53.407096Z",
     "start_time": "2024-06-12T22:00:22.668710Z"
    }
   },
   "source": [
    "# 필요한 라이브러리 설치\n",
    "!pip install torch\n",
    "!pip install torch-scatter torch-sparse torch-cluster torch-spline-conv -f https://data.pyg.org/whl/torch-1.8.0+cu101.html\n",
    "!pip install torch_geometric\n",
    "!pip install scikit-learn\n",
    "!pip install matplotlib\n",
    "!pip install seaborn\n",
    "!pip install imbalanced-learn\n",
    "!pip install optuna\n",
    "\n",
    "# 필요한 라이브러리 임포트\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from torch_geometric.nn import GATConv\n",
    "from sklearn.metrics import mean_absolute_error, r2_score, accuracy_score, confusion_matrix, f1_score, precision_score, recall_score, roc_curve, auc\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from imblearn.over_sampling import SMOTE\n",
    "import optuna\n",
    "\n",
    "# CUDA 설정\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\"\n",
    "os.environ['TORCH_USE_CUDA_DSA'] = \"1\"\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in c:\\users\\researcher\\anaconda3\\lib\\site-packages (2.3.0+cu121)\n",
      "Requirement already satisfied: filelock in c:\\users\\researcher\\anaconda3\\lib\\site-packages (from torch) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in c:\\users\\researcher\\anaconda3\\lib\\site-packages (from torch) (4.9.0)\n",
      "Requirement already satisfied: sympy in c:\\users\\researcher\\anaconda3\\lib\\site-packages (from torch) (1.12)\n",
      "Requirement already satisfied: networkx in c:\\users\\researcher\\anaconda3\\lib\\site-packages (from torch) (3.1)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\researcher\\anaconda3\\lib\\site-packages (from torch) (3.1.3)\n",
      "Requirement already satisfied: fsspec in c:\\users\\researcher\\anaconda3\\lib\\site-packages (from torch) (2023.10.0)\n",
      "Requirement already satisfied: mkl<=2021.4.0,>=2021.1.1 in c:\\users\\researcher\\anaconda3\\lib\\site-packages (from torch) (2021.4.0)\n",
      "Requirement already satisfied: intel-openmp==2021.* in c:\\users\\researcher\\anaconda3\\lib\\site-packages (from mkl<=2021.4.0,>=2021.1.1->torch) (2021.4.0)\n",
      "Requirement already satisfied: tbb==2021.* in c:\\users\\researcher\\anaconda3\\lib\\site-packages (from mkl<=2021.4.0,>=2021.1.1->torch) (2021.11.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\researcher\\anaconda3\\lib\\site-packages (from jinja2->torch) (2.1.3)\n",
      "Requirement already satisfied: mpmath>=0.19 in c:\\users\\researcher\\anaconda3\\lib\\site-packages (from sympy->torch) (1.3.0)\n",
      "Looking in links: https://data.pyg.org/whl/torch-1.8.0+cu101.html\n",
      "Requirement already satisfied: torch-scatter in c:\\users\\researcher\\anaconda3\\lib\\site-packages (2.1.2)\n",
      "Requirement already satisfied: torch-sparse in c:\\users\\researcher\\anaconda3\\lib\\site-packages (0.6.18)\n",
      "Requirement already satisfied: torch-cluster in c:\\users\\researcher\\anaconda3\\lib\\site-packages (1.6.3)\n",
      "Requirement already satisfied: torch-spline-conv in c:\\users\\researcher\\anaconda3\\lib\\site-packages (1.2.2)\n",
      "Requirement already satisfied: scipy in c:\\users\\researcher\\anaconda3\\lib\\site-packages (from torch-sparse) (1.11.4)\n",
      "Requirement already satisfied: numpy<1.28.0,>=1.21.6 in c:\\users\\researcher\\anaconda3\\lib\\site-packages (from scipy->torch-sparse) (1.26.4)\n",
      "Requirement already satisfied: torch_geometric in c:\\users\\researcher\\anaconda3\\lib\\site-packages (2.5.3)\n",
      "Requirement already satisfied: tqdm in c:\\users\\researcher\\anaconda3\\lib\\site-packages (from torch_geometric) (4.65.0)\n",
      "Requirement already satisfied: numpy in c:\\users\\researcher\\anaconda3\\lib\\site-packages (from torch_geometric) (1.26.4)\n",
      "Requirement already satisfied: scipy in c:\\users\\researcher\\anaconda3\\lib\\site-packages (from torch_geometric) (1.11.4)\n",
      "Requirement already satisfied: fsspec in c:\\users\\researcher\\anaconda3\\lib\\site-packages (from torch_geometric) (2023.10.0)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\researcher\\anaconda3\\lib\\site-packages (from torch_geometric) (3.1.3)\n",
      "Requirement already satisfied: aiohttp in c:\\users\\researcher\\anaconda3\\lib\\site-packages (from torch_geometric) (3.9.3)\n",
      "Requirement already satisfied: requests in c:\\users\\researcher\\anaconda3\\lib\\site-packages (from torch_geometric) (2.32.3)\n",
      "Requirement already satisfied: pyparsing in c:\\users\\researcher\\anaconda3\\lib\\site-packages (from torch_geometric) (3.0.9)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\researcher\\anaconda3\\lib\\site-packages (from torch_geometric) (1.2.2)\n",
      "Requirement already satisfied: psutil>=5.8.0 in c:\\users\\researcher\\anaconda3\\lib\\site-packages (from torch_geometric) (5.9.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\researcher\\anaconda3\\lib\\site-packages (from aiohttp->torch_geometric) (1.2.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\researcher\\anaconda3\\lib\\site-packages (from aiohttp->torch_geometric) (23.1.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\researcher\\anaconda3\\lib\\site-packages (from aiohttp->torch_geometric) (1.4.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\researcher\\anaconda3\\lib\\site-packages (from aiohttp->torch_geometric) (6.0.4)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in c:\\users\\researcher\\anaconda3\\lib\\site-packages (from aiohttp->torch_geometric) (1.9.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\researcher\\anaconda3\\lib\\site-packages (from jinja2->torch_geometric) (2.1.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\researcher\\anaconda3\\lib\\site-packages (from requests->torch_geometric) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\researcher\\anaconda3\\lib\\site-packages (from requests->torch_geometric) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\researcher\\anaconda3\\lib\\site-packages (from requests->torch_geometric) (2.0.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\researcher\\anaconda3\\lib\\site-packages (from requests->torch_geometric) (2024.2.2)\n",
      "Requirement already satisfied: joblib>=1.1.1 in c:\\users\\researcher\\anaconda3\\lib\\site-packages (from scikit-learn->torch_geometric) (1.2.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\researcher\\anaconda3\\lib\\site-packages (from scikit-learn->torch_geometric) (2.2.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\researcher\\anaconda3\\lib\\site-packages (from tqdm->torch_geometric) (0.4.6)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\researcher\\anaconda3\\lib\\site-packages (1.2.2)\n",
      "Requirement already satisfied: numpy>=1.17.3 in c:\\users\\researcher\\anaconda3\\lib\\site-packages (from scikit-learn) (1.26.4)\n",
      "Requirement already satisfied: scipy>=1.3.2 in c:\\users\\researcher\\anaconda3\\lib\\site-packages (from scikit-learn) (1.11.4)\n",
      "Requirement already satisfied: joblib>=1.1.1 in c:\\users\\researcher\\anaconda3\\lib\\site-packages (from scikit-learn) (1.2.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\researcher\\anaconda3\\lib\\site-packages (from scikit-learn) (2.2.0)\n",
      "Requirement already satisfied: matplotlib in c:\\users\\researcher\\anaconda3\\lib\\site-packages (3.8.0)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\researcher\\anaconda3\\lib\\site-packages (from matplotlib) (1.2.0)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\researcher\\anaconda3\\lib\\site-packages (from matplotlib) (0.11.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\researcher\\anaconda3\\lib\\site-packages (from matplotlib) (4.25.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in c:\\users\\researcher\\anaconda3\\lib\\site-packages (from matplotlib) (1.4.4)\n",
      "Requirement already satisfied: numpy<2,>=1.21 in c:\\users\\researcher\\anaconda3\\lib\\site-packages (from matplotlib) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\researcher\\anaconda3\\lib\\site-packages (from matplotlib) (23.1)\n",
      "Requirement already satisfied: pillow>=6.2.0 in c:\\users\\researcher\\anaconda3\\lib\\site-packages (from matplotlib) (10.2.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in c:\\users\\researcher\\anaconda3\\lib\\site-packages (from matplotlib) (3.0.9)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\researcher\\anaconda3\\lib\\site-packages (from matplotlib) (2.8.2)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\researcher\\anaconda3\\lib\\site-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\n",
      "Requirement already satisfied: seaborn in c:\\users\\researcher\\anaconda3\\lib\\site-packages (0.12.2)\n",
      "Requirement already satisfied: numpy!=1.24.0,>=1.17 in c:\\users\\researcher\\anaconda3\\lib\\site-packages (from seaborn) (1.26.4)\n",
      "Requirement already satisfied: pandas>=0.25 in c:\\users\\researcher\\anaconda3\\lib\\site-packages (from seaborn) (2.1.4)\n",
      "Requirement already satisfied: matplotlib!=3.6.1,>=3.1 in c:\\users\\researcher\\anaconda3\\lib\\site-packages (from seaborn) (3.8.0)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\researcher\\anaconda3\\lib\\site-packages (from matplotlib!=3.6.1,>=3.1->seaborn) (1.2.0)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\researcher\\anaconda3\\lib\\site-packages (from matplotlib!=3.6.1,>=3.1->seaborn) (0.11.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\researcher\\anaconda3\\lib\\site-packages (from matplotlib!=3.6.1,>=3.1->seaborn) (4.25.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in c:\\users\\researcher\\anaconda3\\lib\\site-packages (from matplotlib!=3.6.1,>=3.1->seaborn) (1.4.4)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\researcher\\anaconda3\\lib\\site-packages (from matplotlib!=3.6.1,>=3.1->seaborn) (23.1)\n",
      "Requirement already satisfied: pillow>=6.2.0 in c:\\users\\researcher\\anaconda3\\lib\\site-packages (from matplotlib!=3.6.1,>=3.1->seaborn) (10.2.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in c:\\users\\researcher\\anaconda3\\lib\\site-packages (from matplotlib!=3.6.1,>=3.1->seaborn) (3.0.9)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\researcher\\anaconda3\\lib\\site-packages (from matplotlib!=3.6.1,>=3.1->seaborn) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\researcher\\anaconda3\\lib\\site-packages (from pandas>=0.25->seaborn) (2023.3.post1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in c:\\users\\researcher\\anaconda3\\lib\\site-packages (from pandas>=0.25->seaborn) (2023.3)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\researcher\\anaconda3\\lib\\site-packages (from python-dateutil>=2.7->matplotlib!=3.6.1,>=3.1->seaborn) (1.16.0)\n",
      "Requirement already satisfied: imbalanced-learn in c:\\users\\researcher\\anaconda3\\lib\\site-packages (0.11.0)\n",
      "Requirement already satisfied: numpy>=1.17.3 in c:\\users\\researcher\\anaconda3\\lib\\site-packages (from imbalanced-learn) (1.26.4)\n",
      "Requirement already satisfied: scipy>=1.5.0 in c:\\users\\researcher\\anaconda3\\lib\\site-packages (from imbalanced-learn) (1.11.4)\n",
      "Requirement already satisfied: scikit-learn>=1.0.2 in c:\\users\\researcher\\anaconda3\\lib\\site-packages (from imbalanced-learn) (1.2.2)\n",
      "Requirement already satisfied: joblib>=1.1.1 in c:\\users\\researcher\\anaconda3\\lib\\site-packages (from imbalanced-learn) (1.2.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\researcher\\anaconda3\\lib\\site-packages (from imbalanced-learn) (2.2.0)\n",
      "Requirement already satisfied: optuna in c:\\users\\researcher\\anaconda3\\lib\\site-packages (3.6.1)\n",
      "Requirement already satisfied: alembic>=1.5.0 in c:\\users\\researcher\\anaconda3\\lib\\site-packages (from optuna) (1.13.1)\n",
      "Requirement already satisfied: colorlog in c:\\users\\researcher\\anaconda3\\lib\\site-packages (from optuna) (6.8.2)\n",
      "Requirement already satisfied: numpy in c:\\users\\researcher\\anaconda3\\lib\\site-packages (from optuna) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\researcher\\anaconda3\\lib\\site-packages (from optuna) (23.1)\n",
      "Requirement already satisfied: sqlalchemy>=1.3.0 in c:\\users\\researcher\\anaconda3\\lib\\site-packages (from optuna) (2.0.25)\n",
      "Requirement already satisfied: tqdm in c:\\users\\researcher\\anaconda3\\lib\\site-packages (from optuna) (4.65.0)\n",
      "Requirement already satisfied: PyYAML in c:\\users\\researcher\\anaconda3\\lib\\site-packages (from optuna) (6.0.1)\n",
      "Requirement already satisfied: Mako in c:\\users\\researcher\\anaconda3\\lib\\site-packages (from alembic>=1.5.0->optuna) (1.3.5)\n",
      "Requirement already satisfied: typing-extensions>=4 in c:\\users\\researcher\\anaconda3\\lib\\site-packages (from alembic>=1.5.0->optuna) (4.9.0)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in c:\\users\\researcher\\anaconda3\\lib\\site-packages (from sqlalchemy>=1.3.0->optuna) (3.0.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\researcher\\anaconda3\\lib\\site-packages (from colorlog->optuna) (0.4.6)\n",
      "Requirement already satisfied: MarkupSafe>=0.9.2 in c:\\users\\researcher\\anaconda3\\lib\\site-packages (from Mako->alembic>=1.5.0->optuna) (2.1.3)\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "cell_type": "code",
   "source": [
    "# 데이터 파일 경로 설정\n",
    "train_files = [\n",
    "    r'C:\\Users\\Researcher\\Desktop\\MTAD-GAT\\train\\train1.csv',\n",
    "    r'C:\\Users\\Researcher\\Desktop\\MTAD-GAT\\train\\train2.csv'\n",
    "]\n",
    "test_files = [\n",
    "    r'C:\\Users\\Researcher\\Desktop\\MTAD-GAT\\test1\\test1.csv',\n",
    "    r'C:\\Users\\Researcher\\Desktop\\MTAD-GAT\\test1\\test2.csv'\n",
    "]\n",
    "\n",
    "# 데이터 로딩 함수\n",
    "def load_data(files):\n",
    "    df_list = []\n",
    "    for file in files:\n",
    "        df = pd.read_csv(file)\n",
    "        df.drop(columns=['time'], inplace=True)  # 'time' 열 제거\n",
    "        df_list.append(df)\n",
    "    combined_df = pd.concat(df_list, axis=0, ignore_index=True)\n",
    "    return combined_df\n",
    "\n",
    "# prepare_data 함수 정의\n",
    "def prepare_data(df, label_column):\n",
    "    inputs = torch.tensor(df.drop(columns=[label_column]).values, dtype=torch.float32)\n",
    "    labels = torch.tensor(df[label_column].values, dtype=torch.long)  # 라벨을 정수형으로 변환\n",
    "    return inputs, labels\n",
    "\n",
    "# edge_index 생성 함수\n",
    "def create_edge_index(num_nodes):\n",
    "    edge_index = []\n",
    "    for i in range(num_nodes):\n",
    "        for j in range(num_nodes):\n",
    "            if i != j:\n",
    "                edge_index.append([i, j])\n",
    "    edge_index = torch.tensor(edge_index, dtype=torch.long).t().contiguous()\n",
    "    return edge_index\n",
    "\n",
    "# 데이터 전처리 함수\n",
    "def preprocess_data(train_files, test_files, label_column='attack'):\n",
    "    print(\"Loading data...\")\n",
    "    train_df = load_data(train_files)\n",
    "    test_df = load_data(test_files)\n",
    "    print(\"Data loaded.\")\n",
    "\n",
    "    scaler = MinMaxScaler()  # MinMaxScaler를 사용하여 데이터 스케일링\n",
    "    train_scaled = scaler.fit_transform(train_df.drop(columns=[label_column]))\n",
    "    test_scaled = scaler.transform(test_df.drop(columns=[label_column]))\n",
    "\n",
    "    train_scaled_df = pd.DataFrame(train_scaled, columns=train_df.columns.drop(label_column))\n",
    "    test_scaled_df = pd.DataFrame(test_scaled, columns=test_df.columns.drop(label_column))\n",
    "\n",
    "    train_scaled_df[label_column] = train_df[label_column].values\n",
    "    test_scaled_df[label_column] = test_df[label_column].values\n",
    "\n",
    "    train_inputs, train_labels = prepare_data(train_scaled_df, label_column)\n",
    "    test_inputs, test_labels = prepare_data(test_scaled_df, label_column)\n",
    "\n",
    "    smote = SMOTE(random_state=42)\n",
    "    train_inputs_smote, train_labels_smote = smote.fit_resample(train_inputs.cpu().numpy(), train_labels.cpu().numpy())\n",
    "    print(\"SMOTE applied.\")\n",
    "\n",
    "    print(f\"train_inputs_smote shape: {train_inputs_smote.shape}, dtype: {train_inputs_smote.dtype}\")\n",
    "    print(f\"train_labels_smote shape: {train_labels_smote.shape}, dtype: {train_labels_smote.dtype}\")\n",
    "    \n",
    "    # 데이터 값 범위 확인\n",
    "    print(f\"train_inputs_smote min: {np.min(train_inputs_smote)}, max: {np.max(train_inputs_smote)}\")\n",
    "    print(f\"train_labels_smote min: {np.min(train_labels_smote)}, max: {np.max(train_labels_smote)}\")\n",
    "\n",
    "    train_inputs_smote = torch.tensor(train_inputs_smote, dtype=torch.float32).to(device)\n",
    "    train_labels_smote = torch.tensor(train_labels_smote, dtype=torch.long).to(device)  # 라벨을 정수형으로 변환\n",
    "\n",
    "    print(f\"train_inputs_smote shape: {train_inputs_smote.shape}, dtype: {train_inputs_smote.dtype}\")\n",
    "    print(f\"train_labels_smote shape: {train_labels_smote.shape}, dtype: {train_labels_smote.dtype}\")\n",
    "\n",
    "    train_dataset_smote = TensorDataset(train_inputs_smote, train_labels_smote)\n",
    "    train_loader_smote = DataLoader(train_dataset_smote, batch_size=32, shuffle=True)\n",
    "\n",
    "    test_inputs, test_labels = test_inputs.to(device), test_labels.to(device)\n",
    "    test_dataset = TensorDataset(test_inputs, test_labels)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "    num_nodes = train_inputs_smote.size(1)\n",
    "    edge_index = create_edge_index(num_nodes)\n",
    "\n",
    "    print(\"Data preprocessed.\")\n",
    "    return train_loader_smote, test_loader, train_df, test_df, edge_index\n",
    "\n",
    "# 데이터 전처리 호출\n",
    "train_loader_smote, test_loader, train_df, test_df, edge_index = preprocess_data(train_files, test_files)\n",
    "print(f\"Train Loader Size: {len(train_loader_smote)}\")\n",
    "print(f\"Test Loader Size: {len(test_loader)}\")\n",
    "print(f\"Edge Index: {edge_index.shape}\")"
   ],
   "metadata": {
    "id": "W6zwsUyKaJwP",
    "ExecuteTime": {
     "end_time": "2024-06-12T22:55:26.056753Z",
     "start_time": "2024-06-12T22:55:20.846454Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      "Data loaded.\n",
      "SMOTE applied.\n",
      "train_inputs_smote shape: (1100048, 62), dtype: float32\n",
      "train_labels_smote shape: (1100048,), dtype: int64\n",
      "train_inputs_smote min: 0.0, max: 1.0\n",
      "train_labels_smote min: 0, max: 1\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: device-side assert triggered\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mRuntimeError\u001B[0m                              Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[56], line 88\u001B[0m\n\u001B[0;32m     85\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m train_loader_smote, test_loader, train_df, test_df, edge_index\n\u001B[0;32m     87\u001B[0m \u001B[38;5;66;03m# 데이터 전처리 호출\u001B[39;00m\n\u001B[1;32m---> 88\u001B[0m train_loader_smote, test_loader, train_df, test_df, edge_index \u001B[38;5;241m=\u001B[39m preprocess_data(train_files, test_files)\n\u001B[0;32m     89\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mTrain Loader Size: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mlen\u001B[39m(train_loader_smote)\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m     90\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mTest Loader Size: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mlen\u001B[39m(test_loader)\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m)\n",
      "Cell \u001B[1;32mIn[56], line 68\u001B[0m, in \u001B[0;36mpreprocess_data\u001B[1;34m(train_files, test_files, label_column)\u001B[0m\n\u001B[0;32m     65\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtrain_inputs_smote min: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mnp\u001B[38;5;241m.\u001B[39mmin(train_inputs_smote)\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m, max: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mnp\u001B[38;5;241m.\u001B[39mmax(train_inputs_smote)\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m     66\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtrain_labels_smote min: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mnp\u001B[38;5;241m.\u001B[39mmin(train_labels_smote)\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m, max: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mnp\u001B[38;5;241m.\u001B[39mmax(train_labels_smote)\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m---> 68\u001B[0m train_inputs_smote \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mtensor(train_inputs_smote, dtype\u001B[38;5;241m=\u001B[39mtorch\u001B[38;5;241m.\u001B[39mfloat32)\u001B[38;5;241m.\u001B[39mto(device)\n\u001B[0;32m     69\u001B[0m train_labels_smote \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mtensor(train_labels_smote, dtype\u001B[38;5;241m=\u001B[39mtorch\u001B[38;5;241m.\u001B[39mlong)\u001B[38;5;241m.\u001B[39mto(device)  \u001B[38;5;66;03m# 라벨을 정수형으로 변환\u001B[39;00m\n\u001B[0;32m     71\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtrain_inputs_smote shape: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mtrain_inputs_smote\u001B[38;5;241m.\u001B[39mshape\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m, dtype: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mtrain_inputs_smote\u001B[38;5;241m.\u001B[39mdtype\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m)\n",
      "\u001B[1;31mRuntimeError\u001B[0m: CUDA error: device-side assert triggered\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"
     ]
    }
   ],
   "execution_count": 56
  },
  {
   "cell_type": "code",
   "source": [
    "\n",
    "# 모델 정의\n",
    "class MTADGAT(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim, num_heads):\n",
    "        super(MTADGAT, self).__init__()\n",
    "        self.gat1_1 = GATConv(input_dim, hidden_dim, heads=num_heads, concat=True)\n",
    "        self.gat1_2 = GATConv(hidden_dim * num_heads, hidden_dim, heads=1, concat=True)\n",
    "        self.gat1_3 = GATConv(hidden_dim, output_dim, heads=1, concat=True)\n",
    "        self.gat2_1 = GATConv(input_dim, hidden_dim, heads=num_heads, concat=True)\n",
    "        self.gat2_2 = GATConv(hidden_dim * num_heads, hidden_dim, heads=1, concat=True)\n",
    "        self.gat2_3 = GATConv(hidden_dim, output_dim, heads=1, concat=True)\n",
    "        self.gru = nn.GRU(output_dim * 2, hidden_dim, batch_first=True)\n",
    "        self.fc_forecasting = nn.Linear(hidden_dim, input_dim)\n",
    "        self.fc_reconstruction = nn.Linear(hidden_dim, input_dim)\n",
    "\n",
    "    def forward(self, x, edge_index1, edge_index2):\n",
    "        x1 = self.gat1_1(x, edge_index1)\n",
    "        x1 = torch.relu(x1)\n",
    "        x1 = self.gat1_2(x1, edge_index1)\n",
    "        x1 = torch.relu(x1)\n",
    "        x1 = self.gat1_3(x1, edge_index1)\n",
    "\n",
    "        x2 = self.gat2_1(x, edge_index2)\n",
    "        x2 = torch.relu(x2)\n",
    "        x2 = self.gat2_2(x2, edge_index2)\n",
    "        x2 = torch.relu(x2)\n",
    "        x2 = self.gat2_3(x2, edge_index2)\n",
    "\n",
    "        x = torch.cat([x1, x2], dim=1)\n",
    "        x = x.view(1, -1, x.size(1))\n",
    "        x, _ = self.gru(x)\n",
    "        x = x.view(-1, x.size(2))\n",
    "\n",
    "        x_forecasting = self.fc_forecasting(x)\n",
    "        x_reconstruction = self.fc_reconstruction(x)\n",
    "\n",
    "        return x_forecasting, x_reconstruction\n",
    "\n",
    "# 손실 함수 및 옵티마이저 정의\n",
    "def get_class_weights(train_df, num_classes):\n",
    "    class_counts = train_df['attack'].value_counts().sort_index()\n",
    "    class_weights = torch.ones(num_classes, dtype=torch.float32).to(device)\n",
    "    for i in range(len(class_counts)):\n",
    "        class_weights[i] = class_counts.max() / class_counts.iloc[i]\n",
    "    return class_weights\n",
    "\n",
    "def get_criteria_and_optimizer(model, class_weights, learning_rate):\n",
    "    criterion_forecasting = nn.CrossEntropyLoss(weight=class_weights)\n",
    "    criterion_reconstruction = nn.MSELoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    return criterion_forecasting, criterion_reconstruction, optimizer\n",
    "\n",
    "\n",
    "# 첫 번째 배치 확인\n",
    "print(\"Checking first batch of train loader:\")\n",
    "for inputs, labels in train_loader_smote:\n",
    "    print(f\"Inputs shape: {inputs.shape}, Labels shape: {labels.shape}\")\n",
    "    break\n",
    "    \n",
    "# 모델 점검\n",
    "input_dim = train_df.shape[1] - 1\n",
    "hidden_dim = 64\n",
    "output_dim = 32\n",
    "num_heads = 8\n",
    "model = MTADGAT(input_dim, hidden_dim, output_dim, num_heads).to(device)\n",
    "print(f\"Model initialized with input_dim={input_dim}, hidden_dim={hidden_dim}, output_dim={output_dim}, num_heads={num_heads}\")\n",
    "\n",
    "# 모델 테스트 함수\n",
    "def test_model(model, inputs, edge_index1, edge_index2):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        try:\n",
    "            print(\"Testing forward pass...\")\n",
    "            outputs_forecasting, outputs_reconstruction = model(inputs, edge_index1.to(device), edge_index2.to(device))\n",
    "            print(f\"Outputs Forecasting shape: {outputs_forecasting.shape}, Outputs Reconstruction shape: {outputs_reconstruction.shape}\")\n",
    "            print(\"Forward pass successful.\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error during model testing: {e}\")\n",
    "\n",
    "# 첫 번째 배치를 사용하여 모델 테스트\n",
    "print(\"Testing model with first batch...\")\n",
    "inputs, labels = next(iter(train_loader_smote))\n",
    "test_model(model, inputs, edge_index, edge_index)"
   ],
   "metadata": {
    "id": "2pI_HedgVJS5",
    "ExecuteTime": {
     "end_time": "2024-06-12T22:55:03.858370Z",
     "start_time": "2024-06-12T22:55:03.703765Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking first batch of train loader:\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: device-side assert triggered\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mRuntimeError\u001B[0m                              Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[54], line 55\u001B[0m\n\u001B[0;32m     53\u001B[0m \u001B[38;5;66;03m# 첫 번째 배치 확인\u001B[39;00m\n\u001B[0;32m     54\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mChecking first batch of train loader:\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m---> 55\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m inputs, labels \u001B[38;5;129;01min\u001B[39;00m train_loader_smote:\n\u001B[0;32m     56\u001B[0m     \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mInputs shape: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00minputs\u001B[38;5;241m.\u001B[39mshape\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m, Labels shape: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mlabels\u001B[38;5;241m.\u001B[39mshape\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m     57\u001B[0m     \u001B[38;5;28;01mbreak\u001B[39;00m\n",
      "File \u001B[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:631\u001B[0m, in \u001B[0;36m_BaseDataLoaderIter.__next__\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m    628\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_sampler_iter \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m    629\u001B[0m     \u001B[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001B[39;00m\n\u001B[0;32m    630\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_reset()  \u001B[38;5;66;03m# type: ignore[call-arg]\u001B[39;00m\n\u001B[1;32m--> 631\u001B[0m data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_next_data()\n\u001B[0;32m    632\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_num_yielded \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;241m1\u001B[39m\n\u001B[0;32m    633\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_dataset_kind \u001B[38;5;241m==\u001B[39m _DatasetKind\u001B[38;5;241m.\u001B[39mIterable \u001B[38;5;129;01mand\u001B[39;00m \\\n\u001B[0;32m    634\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_IterableDataset_len_called \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mand\u001B[39;00m \\\n\u001B[0;32m    635\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_num_yielded \u001B[38;5;241m>\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_IterableDataset_len_called:\n",
      "File \u001B[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:675\u001B[0m, in \u001B[0;36m_SingleProcessDataLoaderIter._next_data\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m    673\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_next_data\u001B[39m(\u001B[38;5;28mself\u001B[39m):\n\u001B[0;32m    674\u001B[0m     index \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_next_index()  \u001B[38;5;66;03m# may raise StopIteration\u001B[39;00m\n\u001B[1;32m--> 675\u001B[0m     data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_dataset_fetcher\u001B[38;5;241m.\u001B[39mfetch(index)  \u001B[38;5;66;03m# may raise StopIteration\u001B[39;00m\n\u001B[0;32m    676\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_pin_memory:\n\u001B[0;32m    677\u001B[0m         data \u001B[38;5;241m=\u001B[39m _utils\u001B[38;5;241m.\u001B[39mpin_memory\u001B[38;5;241m.\u001B[39mpin_memory(data, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_pin_memory_device)\n",
      "File \u001B[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:54\u001B[0m, in \u001B[0;36m_MapDatasetFetcher.fetch\u001B[1;34m(self, possibly_batched_index)\u001B[0m\n\u001B[0;32m     52\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m     53\u001B[0m     data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdataset[possibly_batched_index]\n\u001B[1;32m---> 54\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcollate_fn(data)\n",
      "File \u001B[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py:316\u001B[0m, in \u001B[0;36mdefault_collate\u001B[1;34m(batch)\u001B[0m\n\u001B[0;32m    255\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mdefault_collate\u001B[39m(batch):\n\u001B[0;32m    256\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124mr\u001B[39m\u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[0;32m    257\u001B[0m \u001B[38;5;124;03m    Take in a batch of data and put the elements within the batch into a tensor with an additional outer dimension - batch size.\u001B[39;00m\n\u001B[0;32m    258\u001B[0m \n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    314\u001B[0m \u001B[38;5;124;03m        >>> default_collate(batch)  # Handle `CustomType` automatically\u001B[39;00m\n\u001B[0;32m    315\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[1;32m--> 316\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m collate(batch, collate_fn_map\u001B[38;5;241m=\u001B[39mdefault_collate_fn_map)\n",
      "File \u001B[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py:173\u001B[0m, in \u001B[0;36mcollate\u001B[1;34m(batch, collate_fn_map)\u001B[0m\n\u001B[0;32m    170\u001B[0m transposed \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mlist\u001B[39m(\u001B[38;5;28mzip\u001B[39m(\u001B[38;5;241m*\u001B[39mbatch))  \u001B[38;5;66;03m# It may be accessed twice, so we use a list.\u001B[39;00m\n\u001B[0;32m    172\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(elem, \u001B[38;5;28mtuple\u001B[39m):\n\u001B[1;32m--> 173\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m [collate(samples, collate_fn_map\u001B[38;5;241m=\u001B[39mcollate_fn_map) \u001B[38;5;28;01mfor\u001B[39;00m samples \u001B[38;5;129;01min\u001B[39;00m transposed]  \u001B[38;5;66;03m# Backwards compatibility.\u001B[39;00m\n\u001B[0;32m    174\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m    175\u001B[0m     \u001B[38;5;28;01mtry\u001B[39;00m:\n",
      "File \u001B[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py:173\u001B[0m, in \u001B[0;36m<listcomp>\u001B[1;34m(.0)\u001B[0m\n\u001B[0;32m    170\u001B[0m transposed \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mlist\u001B[39m(\u001B[38;5;28mzip\u001B[39m(\u001B[38;5;241m*\u001B[39mbatch))  \u001B[38;5;66;03m# It may be accessed twice, so we use a list.\u001B[39;00m\n\u001B[0;32m    172\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(elem, \u001B[38;5;28mtuple\u001B[39m):\n\u001B[1;32m--> 173\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m [collate(samples, collate_fn_map\u001B[38;5;241m=\u001B[39mcollate_fn_map) \u001B[38;5;28;01mfor\u001B[39;00m samples \u001B[38;5;129;01min\u001B[39;00m transposed]  \u001B[38;5;66;03m# Backwards compatibility.\u001B[39;00m\n\u001B[0;32m    174\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m    175\u001B[0m     \u001B[38;5;28;01mtry\u001B[39;00m:\n",
      "File \u001B[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py:141\u001B[0m, in \u001B[0;36mcollate\u001B[1;34m(batch, collate_fn_map)\u001B[0m\n\u001B[0;32m    139\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m collate_fn_map \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m    140\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m elem_type \u001B[38;5;129;01min\u001B[39;00m collate_fn_map:\n\u001B[1;32m--> 141\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m collate_fn_map[elem_type](batch, collate_fn_map\u001B[38;5;241m=\u001B[39mcollate_fn_map)\n\u001B[0;32m    143\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m collate_type \u001B[38;5;129;01min\u001B[39;00m collate_fn_map:\n\u001B[0;32m    144\u001B[0m         \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(elem, collate_type):\n",
      "File \u001B[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py:213\u001B[0m, in \u001B[0;36mcollate_tensor_fn\u001B[1;34m(batch, collate_fn_map)\u001B[0m\n\u001B[0;32m    211\u001B[0m     storage \u001B[38;5;241m=\u001B[39m elem\u001B[38;5;241m.\u001B[39m_typed_storage()\u001B[38;5;241m.\u001B[39m_new_shared(numel, device\u001B[38;5;241m=\u001B[39melem\u001B[38;5;241m.\u001B[39mdevice)\n\u001B[0;32m    212\u001B[0m     out \u001B[38;5;241m=\u001B[39m elem\u001B[38;5;241m.\u001B[39mnew(storage)\u001B[38;5;241m.\u001B[39mresize_(\u001B[38;5;28mlen\u001B[39m(batch), \u001B[38;5;241m*\u001B[39m\u001B[38;5;28mlist\u001B[39m(elem\u001B[38;5;241m.\u001B[39msize()))\n\u001B[1;32m--> 213\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m torch\u001B[38;5;241m.\u001B[39mstack(batch, \u001B[38;5;241m0\u001B[39m, out\u001B[38;5;241m=\u001B[39mout)\n",
      "\u001B[1;31mRuntimeError\u001B[0m: CUDA error: device-side assert triggered\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"
     ]
    }
   ],
   "execution_count": 54
  },
  {
   "cell_type": "code",
   "source": [
    "def train_and_evaluate(model, train_loader_smote, test_loader, criterion_forecasting, criterion_reconstruction, optimizer, edge_index1, edge_index2, patience=10):\n",
    "    epochs = 50 \n",
    "    train_losses = []\n",
    "    test_losses = []\n",
    "    precisions = []\n",
    "    recalls = []\n",
    "    roc_aucs = []\n",
    "    accuracies = []\n",
    "\n",
    "    best_roc_auc = 0\n",
    "    best_epoch = 0\n",
    "    epochs_no_improve = 0\n",
    "\n",
    "    print(\"Starting training loop...\")\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        print(f\"Epoch {epoch+1}/{epochs} 시작...\")\n",
    "\n",
    "        for i, (inputs, labels) in enumerate(train_loader_smote):\n",
    "            print(f\"Batch {i+1}/{len(train_loader_smote)} 시작...\")\n",
    "            print(f\"Inputs shape: {inputs.shape}, Labels shape: {labels.shape}\")\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            try:\n",
    "                print(\"Forward pass 시작...\")\n",
    "                outputs_forecasting, outputs_reconstruction = model(inputs, edge_index1.to(device), edge_index2.to(device))\n",
    "                print(f\"Outputs Forecasting shape: {outputs_forecasting.shape}, Outputs Reconstruction shape: {outputs_reconstruction.shape}\")\n",
    "                print(\"Forward pass 완료\")\n",
    "                \n",
    "                print(\"Loss 계산 시작...\")\n",
    "                loss_forecasting = criterion_forecasting(outputs_forecasting, labels.long())\n",
    "                loss_reconstruction = criterion_reconstruction(outputs_reconstruction, inputs)\n",
    "                loss = loss_forecasting + loss_reconstruction\n",
    "                print(f\"Loss 계산 완료: {loss.item():.4f}\")\n",
    "                \n",
    "                print(\"Backward pass 시작...\")\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                print(\"Backward pass 완료\")\n",
    "                \n",
    "                total_loss += loss.item()\n",
    "                print(f\"Batch {i+1}/{len(train_loader_smote)} 완료, Loss: {loss.item():.4f}\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error during training at epoch {epoch}, batch {i}: {e}\")\n",
    "                return\n",
    "\n",
    "        avg_train_loss = total_loss / len(train_loader_smote)\n",
    "        train_losses.append(avg_train_loss)\n",
    "\n",
    "        print(f\"Epoch {epoch+1}/{epochs} Train Loss: {avg_train_loss:.4f}\")\n",
    "\n",
    "        model.eval()\n",
    "        all_preds = []\n",
    "        all_labels = []\n",
    "        total_test_loss = 0\n",
    "        predictions = []\n",
    "        with torch.no_grad():\n",
    "            for i, (inputs, labels) in enumerate(test_loader):\n",
    "                print(f\"Test Batch {i+1}/{len(test_loader)} 시작...\")\n",
    "                inputs, labels = inputs.to(device), labels.to(device)\n",
    "                try:\n",
    "                    outputs_forecasting, outputs_reconstruction = model(inputs, edge_index1.to(device), edge_index2.to(device))\n",
    "                    print(f\"Outputs Forecasting shape: {outputs_forecasting.shape}, Outputs Reconstruction shape: {outputs_reconstruction.shape}\")\n",
    "                    loss_forecasting = criterion_forecasting(outputs_forecasting, labels.long())\n",
    "                    loss_reconstruction = criterion_reconstruction(outputs_reconstruction, inputs)\n",
    "                    loss = loss_forecasting + loss_reconstruction\n",
    "                    total_test_loss += loss.item()\n",
    "                    preds = outputs_forecasting.argmax(dim=1)\n",
    "                    all_preds.extend(preds.cpu().numpy())\n",
    "                    all_labels.extend(labels.cpu().numpy())\n",
    "                    predictions.extend(outputs_forecasting.cpu().numpy().flatten())\n",
    "                    print(f\"Test Batch {i+1}/{len(test_loader)} 완료, Loss: {loss.item():.4f}\")\n",
    "                except Exception as e:\n",
    "                    print(f\"Error during evaluation at epoch {epoch}, batch {i}: {e}\")\n",
    "                    return\n",
    "\n",
    "        avg_test_loss = total_test_loss / len(test_loader)\n",
    "        test_losses.append(avg_test_loss)\n",
    "\n",
    "        print(f\"Epoch {epoch+1}/{epochs} Test Loss: {avg_test_loss:.4f}\")\n",
    "\n",
    "        all_labels = np.array(all_labels)\n",
    "        all_preds = np.array(all_preds)\n",
    "        predictions = np.array(predictions)\n",
    "\n",
    "        precision = precision_score(all_labels, all_preds, average='macro', zero_division=0)\n",
    "        recall = recall_score(all_labels, all_preds, average='macro', zero_division=0)\n",
    "        precisions.append(precision)\n",
    "        recalls.append(recall)\n",
    "\n",
    "        accuracy = accuracy_score(all_labels, all_preds)\n",
    "        accuracies.append(accuracy)\n",
    "\n",
    "        try:\n",
    "            fpr, tpr, _ = roc_curve(all_labels, predictions)\n",
    "            roc_auc = auc(fpr, tpr)\n",
    "            roc_aucs.append(roc_auc)\n",
    "        except ValueError as e:\n",
    "            print(f\"ROC AUC calculation error at epoch {epoch}: {e}\")\n",
    "            roc_auc = 0.0\n",
    "            roc_aucs.append(roc_auc)\n",
    "\n",
    "        print(f\"Epoch {epoch+1}/{epochs} Precision: {precision:.4f}, Recall: {recall:.4f}, Accuracy: {accuracy:.4f}, ROC AUC: {roc_auc:.4f}\")\n",
    "\n",
    "        if roc_auc > best_roc_auc:\n",
    "            best_roc_auc = roc_auc\n",
    "            best_epoch = epoch\n",
    "            torch.save(model.state_dict(), 'best_model.pth')\n",
    "            epochs_no_improve = 0\n",
    "        else:\n",
    "            epochs_no_improve += 1\n",
    "\n",
    "        if epochs_no_improve >= patience:\n",
    "            print(f\"Early stopping triggered at epoch {epoch+1}\")\n",
    "            break\n",
    "\n",
    "    print(\"Loading best model for evaluation...\")\n",
    "    model.load_state_dict(torch.load('best_model.pth'))\n",
    "    test_labels = np.array(all_labels)\n",
    "    predictions = np.array(predictions)\n",
    "    evaluate_model(torch.tensor(test_labels), torch.tensor(predictions), train_losses, test_losses, precisions, recalls, accuracies)\n",
    "\n",
    "    print(\"Training and evaluation completed.\")="
   ],
   "metadata": {
    "id": "f-4tw00BMb7_",
    "ExecuteTime": {
     "end_time": "2024-06-12T22:47:50.674289Z",
     "start_time": "2024-06-12T22:47:50.658928Z"
    }
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "cannot assign to function call (1875901876.py, line 125)",
     "output_type": "error",
     "traceback": [
      "\u001B[1;36m  Cell \u001B[1;32mIn[46], line 125\u001B[1;36m\u001B[0m\n\u001B[1;33m    print(\"Training and evaluation completed.\")=\u001B[0m\n\u001B[1;37m    ^\u001B[0m\n\u001B[1;31mSyntaxError\u001B[0m\u001B[1;31m:\u001B[0m cannot assign to function call\n"
     ]
    }
   ],
   "execution_count": 46
  },
  {
   "cell_type": "code",
   "source": [
    "def evaluate_model(test_labels, predictions, train_losses, test_losses, precisions, recalls, accuracies):\n",
    "    mse = np.mean((test_labels.numpy() - predictions) ** 2)\n",
    "    mae = mean_absolute_error(test_labels.numpy(), predictions)\n",
    "    r2 = r2_score(test_labels.numpy(), predictions)\n",
    "\n",
    "    print(f\"Mean Squared Error: {mse}\")\n",
    "    print(f\"Mean Absolute Error: {mae}\")\n",
    "    print(f\"R^2 Score: {r2}\")\n",
    "\n",
    "    anomaly_threshold = mse + 3 * np.std(predictions - test_labels.numpy().reshape(-1, 1))\n",
    "    anomalies = (np.abs(predictions - test_labels.numpy().reshape(-1, 1)) > anomaly_threshold).astype(int)\n",
    "\n",
    "    accuracy = accuracy_score(test_labels.numpy(), anomalies)\n",
    "    f1 = f1_score(test_labels.numpy(), anomalies, average='macro')\n",
    "    cm = confusion_matrix(test_labels.numpy(), anomalies)\n",
    "\n",
    "    print(f\"Accuracy: {accuracy}\")\n",
    "    print(f\"F1 Score: {f1}\")\n",
    "    print(\"Confusion Matrix:\")\n",
    "    print(cm)\n",
    "\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=['Normal', 'Anomaly'], yticklabels=['Normal', 'Anomaly'])\n",
    "    plt.ylabel('Actual')\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.title('Confusion Matrix')\n",
    "    plt.show()\n",
    "\n",
    "    plt.figure(figsize=(15, 5))\n",
    "    plt.plot(test_labels.numpy(), label='Actual', color='blue')\n",
    "    plt.plot(predictions, label='Predicted', color='green')\n",
    "    plt.scatter(np.where(anomalies == 1)[0], predictions[anomalies == 1], color='red', label='Anomalies')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.plot(train_losses, label='Train Loss')\n",
    "    plt.plot(test_losses, label='Test Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    plt.title('Training and Testing Loss')\n",
    "    plt.show()\n",
    "\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.plot(precisions, label='Precision')\n",
    "    plt.plot(recalls, label='Recall')\n",
    "    plt.plot(accuracies, label='Accuracy')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Score')\n",
    "    plt.legend()\n",
    "    plt.title('Precision, Recall, and Accuracy')\n",
    "    plt.show()\n",
    "\n",
    "    fpr, tpr, _ = roc_curve(test_labels.numpy(), predictions)\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "\n",
    "    plt.figure()\n",
    "    plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (area = {roc_auc:.2f})')\n",
    "    plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title('Receiver Operating Characteristic')\n",
    "    plt.legend(loc='lower right')\n",
    "    plt.show()"
   ],
   "metadata": {
    "id": "LyXrdGM6W6Gr",
    "ExecuteTime": {
     "end_time": "2024-06-12T21:38:11.330096Z",
     "start_time": "2024-06-12T21:38:11.317299Z"
    }
   },
   "outputs": [],
   "execution_count": 59
  },
  {
   "cell_type": "code",
   "source": [
    "# 데이터 전처리 함수\n",
    "def preprocess_data(train_files, test_files, label_column='attack'):\n",
    "    train_df = load_data(train_files)\n",
    "    test_df = load_data(test_files)\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "    train_scaled = scaler.fit_transform(train_df.drop(columns=[label_column]))\n",
    "    test_scaled = scaler.transform(test_df.drop(columns=[label_column]))\n",
    "\n",
    "    train_scaled_df = pd.DataFrame(train_scaled, columns=train_df.columns.drop(label_column))\n",
    "    test_scaled_df = pd.DataFrame(test_scaled, columns=test_df.columns.drop(label_column))\n",
    "\n",
    "    train_scaled_df[label_column] = train_df[label_column].values\n",
    "    test_scaled_df[label_column] = test_df[label_column].values\n",
    "\n",
    "    train_inputs, train_labels = prepare_data(train_scaled_df, label_column)\n",
    "    test_inputs, test_labels = prepare_data(test_scaled_df, label_column)\n",
    "\n",
    "    smote = SMOTE(random_state=42)\n",
    "    train_inputs_smote, train_labels_smote = smote.fit_resample(train_inputs.cpu(), train_labels.cpu())\n",
    "    train_inputs_smote = torch.tensor(train_inputs_smote, dtype=torch.float32).to(device)\n",
    "    train_labels_smote = torch.tensor(train_labels_smote, dtype=torch.float32).to(device)\n",
    "\n",
    "    train_dataset_smote = TensorDataset(train_inputs_smote, train_labels_smote)\n",
    "    train_loader_smote = DataLoader(train_dataset_smote, batch_size=32, shuffle=True)\n",
    "\n",
    "    test_dataset = TensorDataset(test_inputs, test_labels)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "    num_nodes = train_inputs_smote.size(1)\n",
    "    edge_index = create_edge_index(num_nodes)\n",
    "\n",
    "    return train_loader_smote, test_loader, train_df, test_df, edge_index\n",
    "\n",
    "# objective 함수\n",
    "def objective(trial):\n",
    "    train_files = [\n",
    "            r'C:\\Users\\Researcher\\Desktop\\MTAD-GAT\\train\\train1.csv',\n",
    "            r'C:\\Users\\Researcher\\Desktop\\MTAD-GAT\\train\\train2.csv'\n",
    "    ]\n",
    "    test_files = [\n",
    "         r'C:\\Users\\Researcher\\Desktop\\MTAD-GAT\\test1\\test1.csv',\n",
    "         r'C:\\Users\\Researcher\\Desktop\\MTAD-GAT\\test1\\test2.csv'\n",
    "    ]\n",
    "\n",
    "\n",
    "    # 하이퍼파라미터 샘플링\n",
    "    hidden_dim = trial.suggest_int('hidden_dim', 64, 256)\n",
    "    output_dim = trial.suggest_int('output_dim', 64, 256)\n",
    "    num_heads = trial.suggest_int('num_heads', 1, 8)\n",
    "    learning_rate = trial.suggest_float('learning_rate', 1e-5, 1e-2, log=True)\n",
    "    epochs = trial.suggest_int('epochs', 10, 100)\n",
    "\n",
    "    # 데이터 전처리\n",
    "    train_loader_smote, test_loader, train_df, test_df, edge_index = preprocess_data(train_files, test_files)\n",
    "\n",
    "    # 모델 초기화\n",
    "    num_nodes = train_loader_smote.dataset.tensors[0].shape[1]\n",
    "    model = MTADGAT(input_dim=num_nodes, hidden_dim=hidden_dim, output_dim=output_dim, num_heads=num_heads).to(device)\n",
    "\n",
    "    # 손실 함수 및 옵티마이저 설정\n",
    "    num_classes = train_df['attack'].nunique()\n",
    "    class_weights = get_class_weights(train_df, num_classes)\n",
    "    criterion_forecasting, criterion_reconstruction, optimizer = get_criteria_and_optimizer(model, class_weights, learning_rate)\n",
    "\n",
    "    # 모델 학습 및 평가\n",
    "    try:\n",
    "        train_losses, test_losses, precisions, recalls, accuracies, roc_aucs = train_and_evaluate(\n",
    "            model, train_loader_smote, test_loader, criterion_forecasting, criterion_reconstruction, optimizer, edge_index, edge_index, epochs)\n",
    "    except Exception as e:\n",
    "        print(f\"Trial failed due to error: {e}\")\n",
    "        return float('nan')\n",
    "\n",
    "    return max(roc_aucs)  # ROC AUC 최대화\n",
    "\n",
    "study = optuna.create_study(direction='maximize')\n",
    "study.optimize(objective, n_trials=50)\n",
    "\n",
    "# 트라이얼이 완료되었는지 확인\n",
    "if len(study.trials) == 0 or all([t.state != optuna.trial.TrialState.COMPLETE for t in study.trials]):\n",
    "    raise ValueError(\"No trials are completed yet.\")\n",
    "\n",
    "print('Best trial:')\n",
    "trial = study.best_trial\n",
    "print(' Value: ', trial.value)\n",
    "print(' Params: ')\n",
    "for key, value in trial.params.items():\n",
    "    print(f'    {key}: {value}')"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "collapsed": true,
    "id": "jAd1pL88Y0gt",
    "outputId": "4a0d7877-e10b-4343-a854-eb0a7036b58e",
    "ExecuteTime": {
     "end_time": "2024-06-12T21:17:21.623781Z",
     "start_time": "2024-06-12T21:17:21.604971Z"
    }
   },
   "outputs": [],
   "execution_count": 47
  },
  {
   "cell_type": "code",
   "source": [
    "\n",
    "def main():\n",
    "    # 업로드된 파일 경로 설정\n",
    "    train_files = [\n",
    "        '/content/train/train1.csv',\n",
    "        '/content/train/train2.csv'\n",
    "    ]\n",
    "    test_files = [\n",
    "        '/content/test/test1.csv',\n",
    "        '/content/test/test2.csv'\n",
    "    ]\n",
    "\n",
    "    best_params = study.best_params\n",
    "    print(f\"Optimal number of epochs: {best_params['epochs']}\")  # Print the optimal number of epochs\n",
    "\n",
    "    train_loader_smote, test_loader, train_df, test_df = preprocess_data(train_files, test_files)\n",
    "\n",
    "    # edge_index 생성\n",
    "    edge_index1 = create_edge_index(train_df)\n",
    "    edge_index2 = create_edge_index(test_df)\n",
    "\n",
    "    num_nodes = train_loader_smote.dataset.tensors[0].shape[1]\n",
    "    model = MTADGAT(input_dim=num_nodes, hidden_dim=best_params['hidden_dim'], output_dim=best_params['output_dim'], num_heads=best_params['num_heads']).to(device)\n",
    "\n",
    "    num_classes = train_df['attack'].nunique()\n",
    "    class_weights = get_class_weights(train_df, num_classes)\n",
    "    criterion_forecasting, criterion_reconstruction, optimizer = get_criteria_and_optimizer(model, class_weights, best_params['learning_rate'])\n",
    "\n",
    "    train_losses, test_losses, precisions, recalls, accuracies, roc_aucs = train_and_evaluate(\n",
    "        model, train_loader_smote, test_loader, criterion_forecasting, criterion_reconstruction, optimizer, edge_index1, edge_index2, best_params['epochs'])\n",
    "\n",
    "    test_inputs, test_labels = next(iter(test_loader))\n",
    "    test_inputs, test_labels = test_inputs.to(device), test_labels.to(device)\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        outputs_forecasting, _ = model(test_inputs, edge_index1.to(device), edge_index2.to(device))\n",
    "        predictions = outputs_forecasting.argmax(dim=1).cpu().numpy()\n",
    "\n",
    "    evaluate_model(test_labels, predictions, train_losses, test_losses, precisions, recalls, accuracies)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ],
   "metadata": {
    "id": "gMybODnSx1i1",
    "ExecuteTime": {
     "end_time": "2024-06-12T21:17:55.516285Z",
     "start_time": "2024-06-12T21:17:50.594856Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-06-13 06:17:50,594] A new study created in memory with name: no-name-c099f7ca-b192-4d1f-bca9-0b8c838f1f41\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      "Data loaded.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W 2024-06-13 06:17:55,419] Trial 0 failed with parameters: {'hidden_dim': 64, 'output_dim': 111, 'num_heads': 3, 'learning_rate': 0.000418502895230389, 'epochs': 87} because of the following error: RuntimeError('CUDA error: device-side assert triggered\\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\\n').\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Researcher\\anaconda3\\Lib\\site-packages\\optuna\\study\\_optimize.py\", line 196, in _run_trial\n",
      "    value_or_values = func(trial)\n",
      "                      ^^^^^^^^^^^\n",
      "  File \"C:\\Users\\Researcher\\AppData\\Local\\Temp\\ipykernel_6664\\3911277233.py\", line 19, in objective\n",
      "    train_loader_smote, test_loader, train_df, test_df, edge_index = preprocess_data(train_files, test_files)\n",
      "                                                                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\Researcher\\AppData\\Local\\Temp\\ipykernel_6664\\2038594078.py\", line 59, in preprocess_data\n",
      "    train_inputs_smote = torch.tensor(train_inputs_smote, dtype=torch.float32).to(device)\n",
      "                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "RuntimeError: CUDA error: device-side assert triggered\n",
      "Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
      "\n",
      "[W 2024-06-13 06:17:55,419] Trial 0 failed with value None.\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: device-side assert triggered\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mRuntimeError\u001B[0m                              Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[48], line 41\u001B[0m\n\u001B[0;32m     38\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mmax\u001B[39m(roc_aucs)  \u001B[38;5;66;03m# ROC AUC 최대화\u001B[39;00m\n\u001B[0;32m     40\u001B[0m study \u001B[38;5;241m=\u001B[39m optuna\u001B[38;5;241m.\u001B[39mcreate_study(direction\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mmaximize\u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[1;32m---> 41\u001B[0m study\u001B[38;5;241m.\u001B[39moptimize(objective, n_trials\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m50\u001B[39m)\n\u001B[0;32m     43\u001B[0m \u001B[38;5;66;03m# 트라이얼이 완료되었는지 확인\u001B[39;00m\n\u001B[0;32m     44\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(study\u001B[38;5;241m.\u001B[39mtrials) \u001B[38;5;241m==\u001B[39m \u001B[38;5;241m0\u001B[39m \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mall\u001B[39m([t\u001B[38;5;241m.\u001B[39mstate \u001B[38;5;241m!=\u001B[39m optuna\u001B[38;5;241m.\u001B[39mtrial\u001B[38;5;241m.\u001B[39mTrialState\u001B[38;5;241m.\u001B[39mCOMPLETE \u001B[38;5;28;01mfor\u001B[39;00m t \u001B[38;5;129;01min\u001B[39;00m study\u001B[38;5;241m.\u001B[39mtrials]):\n",
      "File \u001B[1;32m~\\anaconda3\\Lib\\site-packages\\optuna\\study\\study.py:451\u001B[0m, in \u001B[0;36mStudy.optimize\u001B[1;34m(self, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001B[0m\n\u001B[0;32m    348\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21moptimize\u001B[39m(\n\u001B[0;32m    349\u001B[0m     \u001B[38;5;28mself\u001B[39m,\n\u001B[0;32m    350\u001B[0m     func: ObjectiveFuncType,\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    357\u001B[0m     show_progress_bar: \u001B[38;5;28mbool\u001B[39m \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mFalse\u001B[39;00m,\n\u001B[0;32m    358\u001B[0m ) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m    359\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"Optimize an objective function.\u001B[39;00m\n\u001B[0;32m    360\u001B[0m \n\u001B[0;32m    361\u001B[0m \u001B[38;5;124;03m    Optimization is done by choosing a suitable set of hyperparameter values from a given\u001B[39;00m\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    449\u001B[0m \u001B[38;5;124;03m            If nested invocation of this method occurs.\u001B[39;00m\n\u001B[0;32m    450\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[1;32m--> 451\u001B[0m     _optimize(\n\u001B[0;32m    452\u001B[0m         study\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m,\n\u001B[0;32m    453\u001B[0m         func\u001B[38;5;241m=\u001B[39mfunc,\n\u001B[0;32m    454\u001B[0m         n_trials\u001B[38;5;241m=\u001B[39mn_trials,\n\u001B[0;32m    455\u001B[0m         timeout\u001B[38;5;241m=\u001B[39mtimeout,\n\u001B[0;32m    456\u001B[0m         n_jobs\u001B[38;5;241m=\u001B[39mn_jobs,\n\u001B[0;32m    457\u001B[0m         catch\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mtuple\u001B[39m(catch) \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(catch, Iterable) \u001B[38;5;28;01melse\u001B[39;00m (catch,),\n\u001B[0;32m    458\u001B[0m         callbacks\u001B[38;5;241m=\u001B[39mcallbacks,\n\u001B[0;32m    459\u001B[0m         gc_after_trial\u001B[38;5;241m=\u001B[39mgc_after_trial,\n\u001B[0;32m    460\u001B[0m         show_progress_bar\u001B[38;5;241m=\u001B[39mshow_progress_bar,\n\u001B[0;32m    461\u001B[0m     )\n",
      "File \u001B[1;32m~\\anaconda3\\Lib\\site-packages\\optuna\\study\\_optimize.py:62\u001B[0m, in \u001B[0;36m_optimize\u001B[1;34m(study, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001B[0m\n\u001B[0;32m     60\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m     61\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m n_jobs \u001B[38;5;241m==\u001B[39m \u001B[38;5;241m1\u001B[39m:\n\u001B[1;32m---> 62\u001B[0m         _optimize_sequential(\n\u001B[0;32m     63\u001B[0m             study,\n\u001B[0;32m     64\u001B[0m             func,\n\u001B[0;32m     65\u001B[0m             n_trials,\n\u001B[0;32m     66\u001B[0m             timeout,\n\u001B[0;32m     67\u001B[0m             catch,\n\u001B[0;32m     68\u001B[0m             callbacks,\n\u001B[0;32m     69\u001B[0m             gc_after_trial,\n\u001B[0;32m     70\u001B[0m             reseed_sampler_rng\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m,\n\u001B[0;32m     71\u001B[0m             time_start\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m,\n\u001B[0;32m     72\u001B[0m             progress_bar\u001B[38;5;241m=\u001B[39mprogress_bar,\n\u001B[0;32m     73\u001B[0m         )\n\u001B[0;32m     74\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m     75\u001B[0m         \u001B[38;5;28;01mif\u001B[39;00m n_jobs \u001B[38;5;241m==\u001B[39m \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m:\n",
      "File \u001B[1;32m~\\anaconda3\\Lib\\site-packages\\optuna\\study\\_optimize.py:159\u001B[0m, in \u001B[0;36m_optimize_sequential\u001B[1;34m(study, func, n_trials, timeout, catch, callbacks, gc_after_trial, reseed_sampler_rng, time_start, progress_bar)\u001B[0m\n\u001B[0;32m    156\u001B[0m         \u001B[38;5;28;01mbreak\u001B[39;00m\n\u001B[0;32m    158\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m--> 159\u001B[0m     frozen_trial \u001B[38;5;241m=\u001B[39m _run_trial(study, func, catch)\n\u001B[0;32m    160\u001B[0m \u001B[38;5;28;01mfinally\u001B[39;00m:\n\u001B[0;32m    161\u001B[0m     \u001B[38;5;66;03m# The following line mitigates memory problems that can be occurred in some\u001B[39;00m\n\u001B[0;32m    162\u001B[0m     \u001B[38;5;66;03m# environments (e.g., services that use computing containers such as GitHub Actions).\u001B[39;00m\n\u001B[0;32m    163\u001B[0m     \u001B[38;5;66;03m# Please refer to the following PR for further details:\u001B[39;00m\n\u001B[0;32m    164\u001B[0m     \u001B[38;5;66;03m# https://github.com/optuna/optuna/pull/325.\u001B[39;00m\n\u001B[0;32m    165\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m gc_after_trial:\n",
      "File \u001B[1;32m~\\anaconda3\\Lib\\site-packages\\optuna\\study\\_optimize.py:247\u001B[0m, in \u001B[0;36m_run_trial\u001B[1;34m(study, func, catch)\u001B[0m\n\u001B[0;32m    240\u001B[0m         \u001B[38;5;28;01massert\u001B[39;00m \u001B[38;5;28;01mFalse\u001B[39;00m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mShould not reach.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    242\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m (\n\u001B[0;32m    243\u001B[0m     frozen_trial\u001B[38;5;241m.\u001B[39mstate \u001B[38;5;241m==\u001B[39m TrialState\u001B[38;5;241m.\u001B[39mFAIL\n\u001B[0;32m    244\u001B[0m     \u001B[38;5;129;01mand\u001B[39;00m func_err \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m    245\u001B[0m     \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(func_err, catch)\n\u001B[0;32m    246\u001B[0m ):\n\u001B[1;32m--> 247\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m func_err\n\u001B[0;32m    248\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m frozen_trial\n",
      "File \u001B[1;32m~\\anaconda3\\Lib\\site-packages\\optuna\\study\\_optimize.py:196\u001B[0m, in \u001B[0;36m_run_trial\u001B[1;34m(study, func, catch)\u001B[0m\n\u001B[0;32m    194\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m get_heartbeat_thread(trial\u001B[38;5;241m.\u001B[39m_trial_id, study\u001B[38;5;241m.\u001B[39m_storage):\n\u001B[0;32m    195\u001B[0m     \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m--> 196\u001B[0m         value_or_values \u001B[38;5;241m=\u001B[39m func(trial)\n\u001B[0;32m    197\u001B[0m     \u001B[38;5;28;01mexcept\u001B[39;00m exceptions\u001B[38;5;241m.\u001B[39mTrialPruned \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[0;32m    198\u001B[0m         \u001B[38;5;66;03m# TODO(mamu): Handle multi-objective cases.\u001B[39;00m\n\u001B[0;32m    199\u001B[0m         state \u001B[38;5;241m=\u001B[39m TrialState\u001B[38;5;241m.\u001B[39mPRUNED\n",
      "Cell \u001B[1;32mIn[48], line 19\u001B[0m, in \u001B[0;36mobjective\u001B[1;34m(trial)\u001B[0m\n\u001B[0;32m     16\u001B[0m epochs \u001B[38;5;241m=\u001B[39m trial\u001B[38;5;241m.\u001B[39msuggest_int(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mepochs\u001B[39m\u001B[38;5;124m'\u001B[39m, \u001B[38;5;241m10\u001B[39m, \u001B[38;5;241m100\u001B[39m)\n\u001B[0;32m     18\u001B[0m \u001B[38;5;66;03m# 데이터 전처리\u001B[39;00m\n\u001B[1;32m---> 19\u001B[0m train_loader_smote, test_loader, train_df, test_df, edge_index \u001B[38;5;241m=\u001B[39m preprocess_data(train_files, test_files)\n\u001B[0;32m     21\u001B[0m \u001B[38;5;66;03m# 모델 초기화\u001B[39;00m\n\u001B[0;32m     22\u001B[0m num_nodes \u001B[38;5;241m=\u001B[39m train_loader_smote\u001B[38;5;241m.\u001B[39mdataset\u001B[38;5;241m.\u001B[39mtensors[\u001B[38;5;241m0\u001B[39m]\u001B[38;5;241m.\u001B[39mshape[\u001B[38;5;241m1\u001B[39m]\n",
      "Cell \u001B[1;32mIn[43], line 59\u001B[0m, in \u001B[0;36mpreprocess_data\u001B[1;34m(train_files, test_files, label_column)\u001B[0m\n\u001B[0;32m     57\u001B[0m smote \u001B[38;5;241m=\u001B[39m SMOTE(random_state\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m42\u001B[39m)\n\u001B[0;32m     58\u001B[0m train_inputs_smote, train_labels_smote \u001B[38;5;241m=\u001B[39m smote\u001B[38;5;241m.\u001B[39mfit_resample(train_inputs\u001B[38;5;241m.\u001B[39mnumpy(), train_labels\u001B[38;5;241m.\u001B[39mnumpy())\n\u001B[1;32m---> 59\u001B[0m train_inputs_smote \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mtensor(train_inputs_smote, dtype\u001B[38;5;241m=\u001B[39mtorch\u001B[38;5;241m.\u001B[39mfloat32)\u001B[38;5;241m.\u001B[39mto(device)\n\u001B[0;32m     60\u001B[0m train_labels_smote \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mtensor(train_labels_smote, dtype\u001B[38;5;241m=\u001B[39mtorch\u001B[38;5;241m.\u001B[39mfloat32)\u001B[38;5;241m.\u001B[39mto(device)\n\u001B[0;32m     62\u001B[0m train_dataset_smote \u001B[38;5;241m=\u001B[39m TensorDataset(train_inputs_smote, train_labels_smote)\n",
      "\u001B[1;31mRuntimeError\u001B[0m: CUDA error: device-side assert triggered\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"
     ]
    }
   ],
   "execution_count": 48
  },
  {
   "cell_type": "code",
   "source": [
    "# 최적 하이퍼파라미터로 모델 학습 및 평가\n",
    "best_params = study.best_params\n",
    "print(f\"Optimal number of epochs: {best_params['epochs']}\")\n",
    "\n",
    "train_loader_smote, test_loader = preprocess_data(train_df, test_df)\n",
    "edge_index1 = create_edge_index(train_df)\n",
    "edge_index2 = create_edge_index(train_df)\n",
    "num_nodes = train_loader_smote.dataset.tensors[0].shape[1]\n",
    "model = MTADGAT(input_dim=num_nodes, hidden_dim=best_params['hidden_dim'], output_dim=best_params['output_dim'], num_heads=best_params['num_heads']).to(device)\n",
    "\n",
    "num_classes = train_df['attack'].nunique()\n",
    "class_weights = get_class_weights(train_df, num_classes)\n",
    "criterion_forecasting, criterion_reconstruction, optimizer = get_criteria_and_optimizer(model, class_weights, best_params['learning_rate'])\n",
    "\n",
    "train_losses, test_losses, precisions, recalls, accuracies, roc_aucs = train_and_evaluate(\n",
    "    model, train_loader_smote, test_loader, criterion_forecasting, criterion_reconstruction, optimizer, edge_index1, edge_index2, best_params['epochs'])\n",
    "\n",
    "print(\"Model training and evaluation with optimal hyperparameters completed.\")"
   ],
   "metadata": {
    "id": "iq1xAl2fx2Iv",
    "ExecuteTime": {
     "end_time": "2024-06-12T21:18:31.851486Z",
     "start_time": "2024-06-12T21:18:31.796035Z"
    }
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "No trials are completed yet.",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mValueError\u001B[0m                                Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[49], line 36\u001B[0m\n\u001B[0;32m     33\u001B[0m     evaluate_model(test_labels, predictions, train_losses, test_losses, precisions, recalls, accuracies)\n\u001B[0;32m     35\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;18m__name__\u001B[39m \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m__main__\u001B[39m\u001B[38;5;124m\"\u001B[39m:\n\u001B[1;32m---> 36\u001B[0m     main()\n",
      "Cell \u001B[1;32mIn[49], line 11\u001B[0m, in \u001B[0;36mmain\u001B[1;34m()\u001B[0m\n\u001B[0;32m      2\u001B[0m train_files \u001B[38;5;241m=\u001B[39m [\n\u001B[0;32m      3\u001B[0m     \u001B[38;5;124mr\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mC:\u001B[39m\u001B[38;5;124m\\\u001B[39m\u001B[38;5;124mUsers\u001B[39m\u001B[38;5;124m\\\u001B[39m\u001B[38;5;124mResearcher\u001B[39m\u001B[38;5;124m\\\u001B[39m\u001B[38;5;124mDesktop\u001B[39m\u001B[38;5;124m\\\u001B[39m\u001B[38;5;124mMTAD-GAT\u001B[39m\u001B[38;5;124m\\\u001B[39m\u001B[38;5;124mtrain\u001B[39m\u001B[38;5;124m\\\u001B[39m\u001B[38;5;124mtrain1.csv\u001B[39m\u001B[38;5;124m'\u001B[39m,\n\u001B[0;32m      4\u001B[0m     \u001B[38;5;124mr\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mC:\u001B[39m\u001B[38;5;124m\\\u001B[39m\u001B[38;5;124mUsers\u001B[39m\u001B[38;5;124m\\\u001B[39m\u001B[38;5;124mResearcher\u001B[39m\u001B[38;5;124m\\\u001B[39m\u001B[38;5;124mDesktop\u001B[39m\u001B[38;5;124m\\\u001B[39m\u001B[38;5;124mMTAD-GAT\u001B[39m\u001B[38;5;124m\\\u001B[39m\u001B[38;5;124mtrain\u001B[39m\u001B[38;5;124m\\\u001B[39m\u001B[38;5;124mtrain2.csv\u001B[39m\u001B[38;5;124m'\u001B[39m\n\u001B[0;32m      5\u001B[0m ]\n\u001B[0;32m      6\u001B[0m test_files \u001B[38;5;241m=\u001B[39m [\n\u001B[0;32m      7\u001B[0m     \u001B[38;5;124mr\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mC:\u001B[39m\u001B[38;5;124m\\\u001B[39m\u001B[38;5;124mUsers\u001B[39m\u001B[38;5;124m\\\u001B[39m\u001B[38;5;124mResearcher\u001B[39m\u001B[38;5;124m\\\u001B[39m\u001B[38;5;124mDesktop\u001B[39m\u001B[38;5;124m\\\u001B[39m\u001B[38;5;124mMTAD-GAT\u001B[39m\u001B[38;5;124m\\\u001B[39m\u001B[38;5;124mtest1\u001B[39m\u001B[38;5;124m\\\u001B[39m\u001B[38;5;124mtest1.csv\u001B[39m\u001B[38;5;124m'\u001B[39m,\n\u001B[0;32m      8\u001B[0m     \u001B[38;5;124mr\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mC:\u001B[39m\u001B[38;5;124m\\\u001B[39m\u001B[38;5;124mUsers\u001B[39m\u001B[38;5;124m\\\u001B[39m\u001B[38;5;124mResearcher\u001B[39m\u001B[38;5;124m\\\u001B[39m\u001B[38;5;124mDesktop\u001B[39m\u001B[38;5;124m\\\u001B[39m\u001B[38;5;124mMTAD-GAT\u001B[39m\u001B[38;5;124m\\\u001B[39m\u001B[38;5;124mtest1\u001B[39m\u001B[38;5;124m\\\u001B[39m\u001B[38;5;124mtest2.csv\u001B[39m\u001B[38;5;124m'\u001B[39m\n\u001B[0;32m      9\u001B[0m ]\n\u001B[1;32m---> 11\u001B[0m best_params \u001B[38;5;241m=\u001B[39m study\u001B[38;5;241m.\u001B[39mbest_params\n\u001B[0;32m     12\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mOptimal number of epochs: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mbest_params[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mepochs\u001B[39m\u001B[38;5;124m'\u001B[39m]\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m     14\u001B[0m train_loader_smote, test_loader, train_df, test_df, edge_index \u001B[38;5;241m=\u001B[39m preprocess_data(train_files, test_files)\n",
      "File \u001B[1;32m~\\anaconda3\\Lib\\site-packages\\optuna\\study\\study.py:114\u001B[0m, in \u001B[0;36mStudy.best_params\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m    102\u001B[0m \u001B[38;5;129m@property\u001B[39m\n\u001B[0;32m    103\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mbest_params\u001B[39m(\u001B[38;5;28mself\u001B[39m) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m \u001B[38;5;28mdict\u001B[39m[\u001B[38;5;28mstr\u001B[39m, Any]:\n\u001B[0;32m    104\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"Return parameters of the best trial in the study.\u001B[39;00m\n\u001B[0;32m    105\u001B[0m \n\u001B[0;32m    106\u001B[0m \u001B[38;5;124;03m    .. note::\u001B[39;00m\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    111\u001B[0m \n\u001B[0;32m    112\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[1;32m--> 114\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mbest_trial\u001B[38;5;241m.\u001B[39mparams\n",
      "File \u001B[1;32m~\\anaconda3\\Lib\\site-packages\\optuna\\study\\study.py:157\u001B[0m, in \u001B[0;36mStudy.best_trial\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m    151\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_is_multi_objective():\n\u001B[0;32m    152\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mRuntimeError\u001B[39;00m(\n\u001B[0;32m    153\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mA single best trial cannot be retrieved from a multi-objective study. Consider \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    154\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124musing Study.best_trials to retrieve a list containing the best trials.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    155\u001B[0m     )\n\u001B[1;32m--> 157\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m copy\u001B[38;5;241m.\u001B[39mdeepcopy(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_storage\u001B[38;5;241m.\u001B[39mget_best_trial(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_study_id))\n",
      "File \u001B[1;32m~\\anaconda3\\Lib\\site-packages\\optuna\\storages\\_in_memory.py:234\u001B[0m, in \u001B[0;36mInMemoryStorage.get_best_trial\u001B[1;34m(self, study_id)\u001B[0m\n\u001B[0;32m    231\u001B[0m best_trial_id \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_studies[study_id]\u001B[38;5;241m.\u001B[39mbest_trial_id\n\u001B[0;32m    233\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m best_trial_id \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m--> 234\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mNo trials are completed yet.\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m    235\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_studies[study_id]\u001B[38;5;241m.\u001B[39mdirections) \u001B[38;5;241m>\u001B[39m \u001B[38;5;241m1\u001B[39m:\n\u001B[0;32m    236\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mRuntimeError\u001B[39;00m(\n\u001B[0;32m    237\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mBest trial can be obtained only for single-objective optimization.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    238\u001B[0m     )\n",
      "\u001B[1;31mValueError\u001B[0m: No trials are completed yet."
     ]
    }
   ],
   "execution_count": 49
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": ""
  }
 ]
}
